\documentclass[conference]{IEEEtran}
\usepackage{times}

% numbers option provides compact numerical references in the text. 
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}
\usepackage{graphicx}
\usepackage{amsmath,amsfonts,amssymb}

\graphicspath{ {./graphics/} }

\pdfinfo{
   /Author (Mingyo Seo)
   /Title  (Robots: Our new overlords)
   /CreationDate (D:20101201120000)
   /Subject (Robots)
   /Keywords (Robots;Overlords)
}

\begin{document}

% paper title
\title{CS391L HW2: Independent Component Analysis}

% You will get a Paper-ID when submitting a pdf file to the conference system
\author{Mingyo Seo}

\author{\authorblockN{Mingyo Seo}
\authorblockA{
UT EID: ms84662\\
Email: mingyo@utexas.edu}}



\maketitle

\IEEEpeerreviewmaketitle

\begin{abstract}

In this assignment, an overview of Independent Component Analysis (ICA) is presented and is applied to separate a mixture of sounds. A set of 5 different sources were taken and mixed to obtain a 5 different "microphone" recordings. The ICA algorithm was applied to retrieve the individual sounds and the retrieval quality was examined.
\end{abstract}

\section{Introduction} % Introduce your problem and the overall plan for approaching your problem

The problem of seperating a certain signal from a mixture of multiple signals can be applied to many areas. The simplest example of this problem is the cocktail party problem where we have recordings from different microphones of the same mixture of conversations.
In this paper, in particular, we implemented an ICA to seperate voices from the mixture of conversations. 
We also studied the effects of parameters on the model's performance. The answers to the HW2 questions are included in the following sections.\begin{itemize}
\item Fig. \ref{fig:base}: Visualization of base source sounds
\item Fig. \ref{fig:mixed}: Visualization of mixed source sounds
\item Fig. \ref{fig:recovered}: Visualization of recovered source sounds
\item Fig. \ref{fig:batch_test}, \ref{fig:eta_test}: Correlation between orignal and recovered sounds
\end{itemize}



\section{Method}
\label{sec:method}

\subsection{Mixing matrix}
In this assignment, a test data set $U$ containing $5$ sound sources and $t$ number of samples is given. We aim to mix $n$ sources from the given sounds to generate a pseudo microphone output $X$ of $m$ recordings where $m\geq n$. To do this, we introduce a mixing matrix $A$, as
\begin{equation}
    X = AU
\end{equation}
Here, $X$ is a $m \times t$ matrix of the mixed sound signals.


\subsection{Independent Component Analysis}

Contrary to Principal Component Analysis which finds the primary features in the given dataset, the ICA method decomposes a mixture of signals into its individual sources. 

The goal of the ICA method is to find the un-mixing $n \times m$ matrix $W$, as
\begin{equation}
    WA = I.
\end{equation}
This yields of restoring the orignial signal $U$, as
\begin{equation}
    U = WX.
\end{equation}
We use the maximum likelihood estimate to find $\hat{W}$, an estimate of $W$, and the gradient descent to approximate it. These methods are described in the following subsections.


\subsection{Maximum Likelihood Estimation}

Computing the unmixing matrix $W$ can be implemented by the singular value decomposition (SVD) which requires computation of the Eigenvalues of $XX^T$, which slows down the speed at large sample recordings $m$.
On the other hand, the Maximum Likelihood Estimation (MLE) method can be used to find the optimal unmixing $W$ that maximize an optimization criteria $L(W)$ to match the mixed signals $X$.

As a criteria  $L(W)$, the following likelihood is used.
\begin{equation}
\begin{aligned}
     p_x(X) &= p_u(U)\cdot|W| \\
    & = \prod_{i=1}p_i(u_i)|W|    
\end{aligned}
\end{equation}
Here, $p_u(U)$ is the probability density function (PDF) of the source signals, and $p_i(u_i)$ is the probability density of the $i_{th}$ source component.
Then, we can rewrite $p_x(X)$ in terms of $t$ mixed samples $x_j: X= \left[x_1, ... , x_t \right]$ and use it as the optimization criteria, as
\begin{equation}
\begin{aligned}
    L(W) = p_x(X) =  \prod_{j=1}^t\prod_{i=1}^np_i(w_i^Tx_j)|W|.
    \label{eq:likelihood def}
\end{aligned}
\end{equation}

From the property that PDF is always positive, we can take log of $L(W)$ to changes the product terms in $W$ into summation terms, as
\begin{equation}
\begin{aligned}
    \ln{(L(W))} 
     &= \sum_{j=1}^t\sum_{i=1}^n\ln{(p_i(w_i^Tx_j)|W|)}\\
     &= \sum_{j=1}^t\sum_{i=1}^n\ln{(p_i(w_i^Tx_j)} + \sum_{i=1}^n\ln(|W|)\\
     &= \sum_{j=1}^t\sum_{i=1}^n\ln{(p_i(w_i^Tx_j)} + t\ln{(|W|)},
     \label{eq:simplified loglike}
\end{aligned}
\end{equation}
which is called {\it log likelihood}. Here, we can maximize $L(W)$ by maximizing the log likelihood. Further, we can rewrite Equation \ref{eq:simplified loglike}, as
\begin{equation}
    \frac{1}{t}\ln{(L(W))} = E\left[\sum_{i=1}^n\ln{(p_i(w_i^Tx_j))}\right] + \ln{(|W|)}.
    \label{eq:log-like-Exp}
\end{equation}
Let $g(X)$ be the cumulative density function (CDF) of the PDF $p_x(X)$.
The CDF $g(X)$ is the integral of $p_x(X)$, so $p(X)$ is the derivative of $g(X)$, as
\begin{equation}
    \frac{1}{t}\ln{(L(W))} = E\left[\ln{(g'(WX)}\right] + \ln{(|W|)}.
    \label{eq:log-like-Exp-cdf} 
\end{equation}


\subsection{Gradient Descent}
The gradient descent method is an iterative algorithm to find a function's minimum or maximum points. If the convex function is convex, the gradient descent converges to the global extremums. Otherwise, it converges to the local extremums. Proving the convexity of the log likelihood $L(W)$ is beyond the scope of this report, so it is not described in this report.

Gradient descent starts at an initial point $\hat{W}_0$ and updates it by moving to next points recursively along the gradient computed at previous points. To formulate the gradient descent method in terms of $W$, the estimate $\hat{W}_{k}$ of the $W$ matrix after the $k^{th}$ iteration is given, as 
\begin{equation}
\begin{aligned}
    \hat{W}_{k+1} &= \hat{W}_k + \eta\cdot\left(\frac{1}{t}\frac{\partial}{\partial W}\ln(L(W))\right)_{W=\hat{W}_k}\\
     &= \hat{W}_k + \eta\cdot\Delta W,
\end{aligned}
\end{equation}
where $\eta$ is the {\it learning rate} of the gradient descent.
Here, the initial point $\hat{W}_0$ is given randomly, and $\eta$ should be adjusted to acheive convergence.

From Equation \ref{eq:log-like-Exp}, we can compute the gradient of $L(W)$, as
\begin{equation}
\begin{aligned}
    \frac{1}{t}\frac{\partial}{\partial W}\ln(L(W)) 
    &= E\left[\frac{\partial}{\partial W}(\ln{(g'(WX)})X^T\right] + \frac{\partial}{\partial W}\ln{(|W|)} \\
    &= E\left[\frac{\partial}{\partial W}(\ln{(g'(WX)})X^T\right] + \left[W^T\right]^{-1}.
\end{aligned}
\end{equation}
Here, the gradient term contains $\left[W^T\right]^{-1}$ which requires expensive computation for the inverse operation. 
Thus, we process the interation by multiplying it with $W^TW$, which preservers the convergence to the optimum, and avoids the inverse operation, as
\begin{equation}
\begin{aligned}
    \Delta W
    &= \frac{1}{t}\frac{\partial}{\partial W}\ln(L(W)) W^T \\
    &= E\left[\frac{\partial}{\partial W}(\ln{(g'(WX)})X^T\right] W^TW + \left[W^T\right]^{-1} W^TW \\
    &= \left(E\left[\left(\frac{\partial}{\partial W}(\ln{(g'(WX)})\right)(WX)^T\right] W + W\right)_{W=\hat{W}_k}.
    \label{eq:delta W simplified}
\end{aligned}
\end{equation}
The above formulation does not affect the  optimality of equation \ref{eq:optimality-eqn}. Decription for this is omitted for brevity of this report, and we encourage to refer the class notes.

To facilitate the gradient descent algorithm, we need to choose a globally differentiable CDF. In particular, we used the following CDF,
\begin{equation}
    g(WX) = \frac{1}{1+e^{-WX}}.
\end{equation}
It is differentiable for all $WX$ and is bounded in $\left[0,1 \right]$. The derivative of $g$ is given as
\begin{equation}
\label{eq:deriv}
    g'(WX) = g(WX)(1-g(WX)).
\end{equation}
Inserting Equation \ref{eq:deriv} into Equation \ref{eq:delta W simplified} yields
\begin{equation}
    \Delta W = \left(\left(E\left[(1-2g(WX))(WX)^T\right] + I \right)W\right)_{W=\hat{W}_k}.
\end{equation}

To summarize the processes described above, the algorithm is implemented, as
\begin{enumerate}
    \item Start with an intial point $\hat{W}_0$.  
    \item Compute matrix $Z_k = g(\hat{W}_kX)$
    \item Compute $\Delta W = \left(E\left[(1-2Z_k)(\hat{W}_kX)^T\right] + I \right)\hat{W}_k$.
    \item Update the estimation as $\hat{W}_{k+1} = \hat{W}_k + \eta\cdot\Delta W$
    \item Return to Step 2 and repeat the processes until it reaches the maximum iteration.
\end{enumerate}


\section{Results} % Describe the methods you intend to apply to solve the given problem

The classifier model of PCA and KNN methods described in Section \ref{sec:method} is implemented python scripts (3.8.5).

\subsection{Model Training}
We trained with the first 1000 samples of the training set, and the result eigenvalues are presented in descending order in Fig. \ref{fig:eig_vals}.
The eigenvectors of the principal components are presented in Fig. \ref{fig:eig_vecs}.
For evaluation of the model, we chose the principal component $M=15$ and KNN size $K=5$, and the model achieves an accuracy of 84.35 \%.


\subsection{Batch size}

The plot of accuracy of with different number of trainign samples is presented in Fig. \ref{fig:sample_test}.
In the experiment, we used $M=24$, and $K=5$.
There is trade-off in using a larger training sample size: the accuracy increases as a training sample size increases but computation time also increases linearly.

\begin{figure}[!t]
	\centering
	\includegraphics[width=3.6in]{source3/batch_test.png}	
	\caption{Accuracy and computation time changes on training sample size: training sample sizes are chosen in [300, 500, 1000, 5000, 10000, 30000, 60000].}
	\label{fig:batch_test}
\end{figure}


\subsection{Learning rate}

The plot of the accuracy of with different KNNs is presented in Fig. \ref{fig:knn_test}.
In the experiment, we used $M=24$, and 1000 training samples.
The accuracy for inferring worsens as KNN size increases, the computation time does not change much.

\begin{figure}[!t]
	\centering
	\includegraphics[width=3.6in]{source3/eta_test.png}	
	\caption{Accuracy and computation time changes on KNN sizes: KNN sizes are chosen in [1, 3, 5, 10, 25].}
	\label{fig:eta_test}
\end{figure}



\section{Summary} %Intermediate/Preliminary Results: State and evaluate your results upeto the milestone
The classifier method of PCA and KNN for classifying digit character images are implemented in the assignment. For evaluation, we used the MNIST dataset. From the results of the experiments, a smaller KNN size benefits the accuracy. However, for the PCA size, the accuracy peaked at $M=50$, This implies that enough number of feature is required for estimation but too many features distract the proper labels, which worsen the accuracy. On the other hand, a larger training size yields higher accuracy but also requires higher computation power.



%\section*{Acknowledgments}

%% Use plainnat to work nicely with natbib. 

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}


